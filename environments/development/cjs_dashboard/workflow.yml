from datetime import datetime
from functools import partial
from airflow.models import DAG
from airflow.operators.email import EmailOperator
from airflow.operators.python import get_current_context
from airflow.operators.python_operator import PythonOperator
from mojap_airflow_tools.operators import BasicKubernetesPodOperator


# As defined in the image repo
IMAGE_TAG = "v14.2.2"
REPO_NAME = "airflow-cjs-dashboard-data"
ENVIRONMENT = "dev"

ROLE = f"airflow_{ENVIRONMENT}_cjs_dashboard_data"
SERVICE_ACCOUNT_NAME = ROLE.replace("_", "-")

# Get timestamp of when process executes on airflow
EXECUTION_TIME = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# List of data providers to run through validation process
data_providers = [
    "ho",
    "cps",
    "moj",
    "hmcts"
]

# For tips/advice on the default args see the use_dummy_operator.py example
default_args = {
    # Normally you want to set this to False.
    "depends_on_past": False,
    "email_on_failure": False,
    # Name of DAG owner as it will appear on Airflow UI
    "owner": "RolakeO-mojo",
    "retries": 2,
    "retry_delay": 60
}

dag = DAG(
    # Name of the dag (how it will appear on the Airflow UI)
    # We use the naming convention: <folder_name>.<filename>
    dag_id="cjs_dashboard.r_validation",
    default_args=default_args,
    description="A DAG to execute CJS Dashboard data validation process in R",
    # Requires a start_date as a datetime object. This will be when the
    # DAG will start running from. DO NOT use datetime.now().
    start_date=datetime(2025, 6, 20),
    end_date=datetime(2025, 6, 21),
    # Schedule for the dag to run.
    # For example, to run every 30 minutes between 09:00 and 17:59 UTC/GMT, set
    # schedule_interval="*/30 9-17 * * 1-5"
    # If set to None, the dag will only run when manually triggered (but start_date and
    # end_date will still apply).
    schedule_interval="*/30 9-17 * * 1-5"
)


# Define function to handle email operator, uses xcom to pull timestamp information
# from r validation process in task 1 (below)
def _send_email(data_provider):
    context = get_current_context()
    returned = context.get('task_instance')\
                      .xcom_pull(task_ids=f'r-validation-{data_provider}')
    max_timestamp = returned["max_timestamp"]
    result = returned["overall_result"]
    task_id = f"send-email-{data_provider}"
    email_addresses = ["rolake.odebunmi@justice.gov.uk",
                       "Joe.Dods@justice.gov.uk"]
    subject = f"New data validation output ready - {data_provider.upper()} - {result}"
    # HTML contents of email
    email_contents = ("<h3>A new data validation output has been created</h3>"
                      f"Data provider: {data_provider.upper()}<br>"
                      f"Extraction max_timestamp: {max_timestamp}<br>"
                      f"Result: {result}"
                      "<p>Detailed results saved in "
                      f"s3://alpha-cjs-scorecard/data_validation/prod/{data_provider}/{max_timestamp}/</p>"
                      )
    email_task = EmailOperator(
        to=email_addresses,
        subject=subject,
        html_content=email_contents,
        dag=dag,
        task_id=task_id
    )
    return email_task.execute(context=context)


def send_email(data_provider):
    return partial(_send_email, data_provider)


# Runs tasks for data providers simultaneously
for data_provider in data_providers:
    # Environmental variables for passing to the docker container
    env_vars = {
        "DATA_PROVIDER": data_provider,
        "ENVIRONMENT": ENVIRONMENT,
        "EXECUTION_TIME": EXECUTION_TIME,
        "AWS_METADATA_SERVICE_TIMEOUT": "60",
        "AWS_METADATA_SERVICE_NUM_ATTEMPTS": "5"
    }

    # Task 1 runs the data validation process for the given data provider
    task_id_1 = f"r-validation-{data_provider}"
    task_1 = BasicKubernetesPodOperator(
        dag=dag,
        repo_name=REPO_NAME,
        release=IMAGE_TAG,
        role=ROLE,
        service_account_name=SERVICE_ACCOUNT_NAME,
        environment="dev",
        task_id=task_id_1,
        env_vars=env_vars,
        do_xcom_push=True
    )

    # If Task 1 succeeds, Task 2 will call the send_email function
    email_func = send_email(data_provider)
    task_id_2 = f"python-send-email-{data_provider}"
    task_2 = PythonOperator(
        dag=dag,
        task_id=task_id_2,
        python_callable=email_func,
        provide_context=True
    )

    task_1 >> task_2
