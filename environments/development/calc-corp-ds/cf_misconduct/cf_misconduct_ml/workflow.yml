from datetime import datetime

from airflow.models import DAG
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import (
    KubernetesPodOperator,
)
from imports.high_memory_constants import tolerations, affinity

# It is good practice to define the image tag and repo name
# as constants in your script. And then derive the full IMAGE name below.
# This avoids making typos on the part of the IMAGE that will most likely
# not change.

IMAGE_TAG = "v0.0.76"

REPO_NAME = "airflow-cf-misconduct"
IMAGE = f"189157455002.dkr.ecr.eu-west-1.amazonaws.com/{REPO_NAME}:{IMAGE_TAG}"

ROLE = "airflow_dev_cf_misconduct"

# For tips/advice on the default args see the use_dummy_operator.py example
default_args = {
    # Normally you want to set this to False.
    "depends_on_past": False,
    "email_on_failure": True,
    "owner": "jhenfrancis",
    "email": ['jhenielle.francis@justice.gov.uk'],
}

dag = DAG(
    # Name of the dag (how it will appear on the Airflow UI)
    # We use the naming convention: <folder_name>.<filename>
    dag_id="cf_misconduct.cf_misconduct_ml",
    default_args=default_args,
    description="A Kubernetes DAG with high memory node",
    # Requires a start_date as a datetime object. This will be when the
    # DAG will start running from. DO NOT use datetime.now().
    start_date=datetime(2022, 1, 1),
    # How often should I run the dag. If set to None your dag
    # will only run when manually triggered.
    schedule_interval=None,
)

tasks = {}

task_id = "model_tuning"
tasks[task_id] = KubernetesPodOperator(
    dag=dag,
    image=IMAGE,
    # Some useful default ENV arguments for your docker image
    env_vars={
        "PYTHON_SCRIPT_NAME": "tuning.py",
        "AWS_METADATA_SERVICE_TIMEOUT": "60",
        "AWS_METADATA_SERVICE_NUM_ATTEMPTS": "5",
        "AWS_DEFAULT_REGION": "eu-west-1",
    },
    name=task_id,
    in_cluster=False,
    task_id=task_id,
    get_logs=True,
    is_delete_operator_pod=True,
    namespace="airflow",
    annotations={"iam.amazonaws.com/role": ROLE},
    cluster_context="aws",
    config_file="/usr/local/airflow/dags/.kube/config",
    security_context={
        "runAsNonRoot": True,
        "runAsUser": 1000,
        "allowPrivilegeEscalation": False,
        "privileged": False,
    },
    # Increase startup time to give time for node creation
    startup_timeout_seconds=600,
    tolerations=tolerations,
    affinity=affinity,
)

task_id = "create_predictions"
tasks[task_id] = KubernetesPodOperator(
    dag=dag,
    image=IMAGE,
    # Some useful default ENV arguments for your docker image
    env_vars={
        "PYTHON_SCRIPT_NAME": "create_pred.py",
        "AWS_METADATA_SERVICE_TIMEOUT": "60",
        "AWS_METADATA_SERVICE_NUM_ATTEMPTS": "5",
        "AWS_DEFAULT_REGION": "eu-west-1",
    },
    name=task_id,
    in_cluster=False,
    task_id=task_id,
    get_logs=True,
    is_delete_operator_pod=True,
    namespace="airflow",
    annotations={"iam.amazonaws.com/role": ROLE},
    cluster_context="aws",
    config_file="/usr/local/airflow/dags/.kube/config",
    security_context={
        "runAsNonRoot": True,
        "runAsUser": 1000,
        "allowPrivilegeEscalation": False,
        "privileged": False,
    },
    # Increase startup time to give time for node creation
    startup_timeout_seconds=600,
    tolerations=tolerations,
    affinity=affinity,
)

tasks["model_tuning"] >> tasks["create_predictions"]
